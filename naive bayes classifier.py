# -*- coding: utf-8 -*-
"""Kempok 13 Naive bayes classifier .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/158e3eeDeozlig9bQaAdQ3PrJsLCufWMN

1. SCRAPING DATA
"""

pip install google-play-scraper #insntall scraper google play

from google_play_scraper import app
import pandas as pd
import numpy as np

from google_play_scraper import Sort, reviews

result, continuation_token = reviews(
    'id.dana',
    lang='id',
    country='id',
    sort=Sort.MOST_RELEVANT,
    count = 1000,
    filter_score_with=None
)

df_busu = pd.DataFrame(np.array(result),columns=['review'])
df_busu = df_busu.join(pd.DataFrame(df_busu.pop('review').tolist()))
df_busu.head()

len(df_busu.index)

df_busu[['score', 'content' ]]

my_df = df_busu [['score', 'content']]

"""2. PELEBELAN"""

def pelabelan(score):
  if score < 3:
    return 'Negatif'
  elif score == 4 :
    return 'Positif'
  elif score == 5 :
    return 'Positif'
my_df ['label'] = my_df ['score'].apply(pelabelan)
my_df.head(50)

my_df.to_csv("scrapped_data.csv", index = False)

import pandas as pd
pd.set_option('display.max_columns', None)
my_df = pd.read_csv('/content/scrapped_data.csv')
my_df.head(50)

"""3. DATA CLEANING (PEMBERSIHAN DATA)"""

my_df.info()

my_df.isna()

my_df.isna().any()

my_df.isnull().sum()

my_df.dropna(subset=['label'],inplace = True)

my_df.isnull().sum()

my_df.head(50)

my_df.to_csv("Danapreprocessing.csv", index = False)

import pandas as pd
df = pd.read_csv('/content/Danapreprocessing.csv')
df.head(50)

"""4. CASE HOLDING"""

import re #melakukan operasi regular expression (regex).
def  clean_text(df, text_field, new_text_field_name):
    my_df[new_text_field_name] = my_df[text_field].str.lower() #mengubahnya menjadi huruf kecil semua,
    my_df[new_text_field_name] = my_df[new_text_field_name].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))# Menghapus karakter
    my_df[new_text_field_name] = my_df[new_text_field_name].apply(lambda elem: re.sub(r"\d+", "", elem))#menghapus angka
    return my_df

my_df['text_clean'] = my_df['content'].str.lower()
my_df['text_clean']
data_clean = clean_text(my_df, 'content', 'text_clean')
data_clean.head(10)

"""5. FILTERING"""

#stopwoed removal
import nltk.corpus
nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('indonesian')
data_clean['text_StopWord'] = data_clean['text_clean'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))
data_clean.head(50)

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize # word_tokenize digunakan untuk memecah teks menjadi token kata.
data_clean['text_tokens'] = data_clean['text_StopWord'].apply(lambda x: word_tokenize(x))
data_clean.head()

"""6. STEMMING"""

pip install Sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()

#-----------------STEMMING -----------------
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
#mengimpor lebih cepat


# membuat stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# stemmed
def stemmed_wrapper(term):
    return stemmer.stem(term)

term_dict = {}
hitung=0

for document in data_clean['text_tokens']:
    for term in document:
        if term not in term_dict:
            term_dict[term] = ' '

print(len(term_dict))
print("------------------------")
for term in term_dict:
    term_dict[term] = stemmed_wrapper(term)
    hitung+=1
    print(hitung,":",term,":" ,term_dict[term])

print(term_dict)
print("------------------------")

# menerapkan stemmed term  dataframe
def get_stemmed_term(document):
    return [term_dict[term] for term in document]


#script ini bisa dipisah dari eksekusinya setelah pembacaaan term selesai
data_clean['text_steamindo'] = data_clean['text_tokens'].apply(lambda x:' '.join(get_stemmed_term(x)))
data_clean.head(20)

data_clean.to_csv('hasil_TextPreProcessing_Dana.csv', index= False)

"""7. PEMBOBOTAN TF-IDF"""

#disini kita importkan library re, kemudian kita lakukan praproses
import re
def praproses(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)()(?:-)?(?:\)|\(|D|P)', #menggunakan ekspresi reguler untuk menemukan emotikon dalam teks
                           text)
    text = (re.sub('[\W]+', ' ', text.lower()) + #karakter (\W) dengan spasi dan mengonversi teks menjadi huruf kecil.
            ' '.join(emoticons).replace('-', '')) #emotikon yang telah ditemukan dijadikan satu string dengan spasi sebagai pemisah.

    return text

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_clean['content'], data_clean['label'],
                                                    test_size = 0.20,
                                                    random_state = 0)

#pembobotan tf-idf
from sklearn.feature_extraction.text import TfidfVectorizer #mengonversi kumpulan dokumen teks menjadi matriks fitur TF-IDF.

tfidf_vectorizer = TfidfVectorizer()

tfidf_train = tfidf_vectorizer.fit_transform(X_train)
tfidf_test = tfidf_vectorizer.transform(X_test)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
vectorizer.fit(X_train) # menghitung setiap kata dalam dataset

X_train = vectorizer.transform(X_train)
X_test = vectorizer.transform(X_test)

"""8. KLASIFIKASI NAIVE BAYES"""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(tfidf_train, y_train) #mengklasifikasikan text verdasarkan matrixt fitur tfidf yang telah dihasilkan sebelumnya

X_train.toarray()

y_pred = nb.predict(tfidf_test)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

#  mendefinisikan X_train, y_train, X_test, dan y_test
# Inisialisasi model klasifikasi Naive Bayes Multinomial

clf = MultinomialNB()
clf.fit(X_train, y_train) # Melatih model
predicted = clf.predict(X_test) # Melakukan prediksi pada data uji

# Mencetak beberapa metrik kinerja
print("MultinomialNB Accuracy:", accuracy_score(y_test,predicted))
print("MultinomialNB Precision:", precision_score(y_test,predicted, average="micro", pos_label="Negatif"))
print("MultinomialNB Recall:", recall_score(y_test,predicted, average="micro", pos_label="Negatif"))
print("MultinomialNB f1_score:", f1_score(y_test,predicted, average="micro", pos_label="Negatif"))

# Mencetak laporan klasifikasi
print(f'confusion_matrix:\n {confusion_matrix(y_test, predicted)}')
print('====================================================\n')
print(classification_report(y_test, predicted, zero_division=0))

# Load dataset
data_clean = pd.read_csv('hasil_TextPreProcessing_Dana.csv')